{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186ae41e-4d85-4a23-87fe-2e1f986b1368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([6, 1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "\n",
    "# ConvLSTMCell类定义\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_channels + self.hidden_channels,\n",
    "                              out_channels=4 * self.hidden_channels,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        combined_conv = F.elu(combined_conv)  # Applying ELU activation\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_channels, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        device = next(self.parameters()).device\n",
    "        return (torch.zeros(batch_size, self.hidden_channels, height, width, device=device),\n",
    "                torch.zeros(batch_size, self.hidden_channels, height, width, device=device))\n",
    "\n",
    "# ConvEncoder类定义\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size, dropout_prob=0.7):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        self.conv_lstm1 = ConvLSTMCell(input_channels, hidden_channels // 2, kernel_size)\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_channels // 2)\n",
    "        self.dropout = nn.Dropout2d(dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        batch_size, seq_len, _, height, width = input_tensor.size()\n",
    "        h, c = self.conv_lstm1.init_hidden(batch_size, (height, width))\n",
    "        layer_output_list = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.conv_lstm1(input_tensor[:, t, :, :, :], (h, c))\n",
    "            h = self.batch_norm(h)\n",
    "            h = self.dropout(h)\n",
    "            layer_output_list.append(h)\n",
    "\n",
    "        layer_output = torch.stack(layer_output_list, dim=1)\n",
    "        return layer_output, (h, c)\n",
    "\n",
    "# ConvDecoder类定义\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, output_channels, kernel_size, dropout_prob=0.7):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        self.conv_lstm1 = ConvLSTMCell(hidden_channels // 2, hidden_channels // 2, kernel_size)\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_channels // 2)\n",
    "        self.conv = nn.Conv2d(hidden_channels // 2, output_channels, kernel_size=1)\n",
    "        self.dropout = nn.Dropout2d(dropout_prob)\n",
    "\n",
    "    def forward(self, encoder_output, h, c, seq_len):\n",
    "        batch_size, seq_len_enc, hidden_channels, height, width = encoder_output.size()\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            # Reshape and calculate attention weights\n",
    "            h_reshaped = h.view(batch_size, -1)\n",
    "            encoder_output_reshaped = encoder_output.view(batch_size, seq_len_enc, -1)\n",
    "            attention_weights = torch.bmm(encoder_output_reshaped, h_reshaped.unsqueeze(2)).squeeze(2)\n",
    "            attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "            attention_applied = torch.bmm(attention_weights.unsqueeze(1), encoder_output_reshaped).squeeze(1)\n",
    "            attention_applied = attention_applied.view(batch_size, hidden_channels, height, width)\n",
    "\n",
    "            # LSTM cell update\n",
    "            h, c = self.conv_lstm1(attention_applied, (h, c))\n",
    "            h = self.batch_norm(h)\n",
    "            h = self.dropout(h)\n",
    "            outputs.append(h)\n",
    "\n",
    "        output = self.conv(outputs[-1])\n",
    "        output = output.unsqueeze(1)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Seq2SeqAutoencoder类定义\n",
    "class Seq2SeqAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, output_channels, kernel_size):\n",
    "        super(Seq2SeqAutoencoder, self).__init__()\n",
    "        self.encoder = ConvEncoder(input_channels, hidden_channels, kernel_size)\n",
    "        self.decoder = ConvDecoder(hidden_channels, output_channels, kernel_size)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        encoder_output, (h, c) = self.encoder(input_tensor)\n",
    "        output = self.decoder(encoder_output, h, c, seq_len=1)\n",
    "        return output\n",
    "\n",
    "# Model initialization\n",
    "input_channels = 1\n",
    "hidden_channels = 128\n",
    "output_channels = 1\n",
    "kernel_size = 3\n",
    "sequence_length = 6\n",
    "\n",
    "model = Seq2SeqAutoencoder(input_channels, hidden_channels, output_channels, kernel_size)\n",
    "\n",
    "# 数据集类定义\n",
    "class TyphoonDataset(Dataset):\n",
    "    def __init__(self, image_paths, sequence_length=6, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_paths = self.image_paths[idx:idx + self.sequence_length]\n",
    "        imgs = [Image.open(img_path).convert('RGB') for img_path in sequence_paths]\n",
    "        if self.transform:\n",
    "            imgs = [self.transform(img) for img in imgs]\n",
    "        sequences = torch.stack(imgs[:-1])\n",
    "        target = imgs[-1]  # 确保目标图像是 [1, height, width]\n",
    "        return sequences, target\n",
    "\n",
    "# 加载并排序图像文件\n",
    "def load_images_sorted(directory):\n",
    "    images = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for fname in files:\n",
    "            if fname.endswith('.jpg'):\n",
    "                full_path = os.path.join(root, fname)\n",
    "                images.append(full_path)\n",
    "    images.sort(key=extract_number)\n",
    "    return images\n",
    "\n",
    "# 从文件名中提取数字的辅助函数\n",
    "def extract_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "# 从文件名中提取风暴ID和时间的辅助函数\n",
    "def extract_storm_id_and_time(filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    parts = filename.split('_')\n",
    "    storm_id = parts[0]\n",
    "    time = parts[1].split('.')[0]\n",
    "    return storm_id, time\n",
    "\n",
    "# 提取所有图像信息的辅助函数\n",
    "def extract_all_image_info(image_paths):\n",
    "    all_image_info = []\n",
    "    for path in image_paths:\n",
    "        storm_id, time = extract_storm_id_and_time(path)\n",
    "        all_image_info.append((storm_id, time))\n",
    "    return all_image_info\n",
    "\n",
    "# 解压缩文件\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "# 数据目录\n",
    "zip_path = '/root/data/storm.zip'\n",
    "extract_to = '/root/data/stormoutput'\n",
    "extract_zip(zip_path, extract_to)\n",
    "\n",
    "all_images = load_images_sorted(extract_to)\n",
    "\n",
    "# 按风暴ID分组\n",
    "storm_groups = defaultdict(list)\n",
    "for img in all_images:\n",
    "    storm_id, _ = extract_storm_id_and_time(img)\n",
    "    storm_groups[storm_id].append(img)\n",
    "\n",
    "# 分别对每个风暴类型按照比例进行划分\n",
    "train_images = []\n",
    "val_images = []\n",
    "split_ratio = 0.8\n",
    "\n",
    "for storm_id, images in storm_groups.items():\n",
    "    split_point = int(len(images) * split_ratio)\n",
    "    train_images.extend(images[:split_point])\n",
    "    val_images.extend(images[split_point:])\n",
    "\n",
    "# 提取训练集和验证集的所有图片信息\n",
    "train_image_info = extract_all_image_info(train_images)\n",
    "val_image_info = extract_all_image_info(val_images)\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TyphoonDataset(train_images, sequence_length=6)  # 包含6个输入帧和1个输出帧\n",
    "val_dataset = TyphoonDataset(val_images, sequence_length=6)\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, shuffle=False, drop_last=True)\n",
    "\n",
    "# 测试模型\n",
    "for sequences, target in train_loader:\n",
    "    output = model(sequences)\n",
    "    print(\"Output shape:\", output.shape)  # 检查输出形状\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19382d87-cce1-43e5-94a9-76418149f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = Seq2SeqAutoencoder(input_channels=1, hidden_channels=128, output_channels=1, kernel_size=3)\n",
    "model.cuda()  # 如果有GPU\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.05)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "num_epochs = 10  # 设置所需的epoch数\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for sequences, targets in train_loader:\n",
    "        sequences = sequences.cuda()\n",
    "        targets = targets.cuda()  # 确保目标维度是正确的 [batch_size, 1, height, width]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        train_loss = criterion(outputs, targets)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += train_loss.item()\n",
    "    train_losses.append(total_train_loss / len(train_loader))\n",
    "    \n",
    "    scheduler.step()  # Step the scheduler\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in val_loader:\n",
    "            sequences = sequences.cuda()\n",
    "            targets = targets.cuda()\n",
    "            outputs = model(sequences)\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            total_val_loss += val_loss.item()\n",
    "    val_losses.append(total_val_loss / len(val_loader))\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {total_train_loss / len(train_loader)}, Validation Loss: {total_val_loss / len(val_loader)}')\n",
    "\n",
    "# 保存loss曲线图\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curve')\n",
    "plt.savefig('/root/fangan4/loss_curve.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0d0b8-7d07-476b-9ffb-df86992ae227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, '/root/fangan4/model4.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
